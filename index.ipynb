{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as ss, pandas as pd, numpy as np\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 15\n",
    "SEED = 1\n",
    "\n",
    "class DictDist():\n",
    "    def __init__(self, dict_of_rvs): self.dict_of_rvs = dict_of_rvs\n",
    "    def rvs(self, n):\n",
    "        a = {k: v.rvs(n) for k, v in self.dict_of_rvs.items()}\n",
    "        out = []\n",
    "        for i in range(n): out.append({k: vs[i] for k, vs in a.items()})\n",
    "        return out\n",
    "    \n",
    "class Choice():\n",
    "    def __init__(self, options): self.options = options\n",
    "    def rvs(self, n): return [self.options[i] for i in ss.randint(0, len(self.options)).rvs(n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Prepare a list of ```{parameters : values}``` from a predetermined set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "################### CLASSIFICATION #####################\n",
    "########################################################\n",
    "# Logisitc Regression\n",
    "LR_dist = DictDist({\n",
    "    'C': Choice(np.geomspace(1e-3, 1e3, 10000)),\n",
    "    'penalty': Choice(['l1', 'l2']),\n",
    "    'solver': Choice(['lbfgs']),\n",
    "    'max_iter': Choice([500])\n",
    "})\n",
    "np.random.seed(SEED)\n",
    "LR_hyperparams_list = LR_dist.rvs(N)\n",
    "for i in range(N):\n",
    "    if LR_hyperparams_list[i]['solver'] == 'lbfgs': LR_hyperparams_list[i]['penalty'] = 'l2'\n",
    "\n",
    "# Random Forest\n",
    "RF_dist = DictDist({\n",
    "    'n_estimators': ss.randint(50, 500),\n",
    "    'max_depth': ss.randint(2, 10),\n",
    "    'min_samples_split': ss.randint(2, 75),\n",
    "    'min_samples_leaf': ss.randint(1, 50),\n",
    "})\n",
    "np.random.seed(SEED)\n",
    "RF_hyperparams_list = RF_dist.rvs(N)\n",
    "\n",
    "# Support Vector\n",
    "SV_dist = DictDist({\n",
    "    'C': Choice([0.1]),\n",
    "    'class_weight': Choice(['balanced']), \n",
    "    'verbose': Choice([0]),\n",
    "    'probability': Choice([True]),\n",
    "})\n",
    "np.random.seed(SEED)\n",
    "SV_hyperparams_list = SV_dist.rvs(1)\n",
    "\n",
    "\n",
    "########################################################\n",
    "##################### PREDICTION #######################\n",
    "########################################################\n",
    "# XGBRegressor\n",
    "XG_dist = DictDist({\n",
    "    'n_estimators': ss.randint(50, 500),\n",
    "    'eta': Choice([0.1]),\n",
    "    'verbosity': Choice([0]),\n",
    "    'max_depth': ss.randint(2, 10),\n",
    "})\n",
    "np.random.seed(SEED)\n",
    "XG_hyperparams_list = XG_dist.rvs(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "\n",
    "x_train = pd.read_csv('../data/X_train.csv', index_col=0, header=[0, 1, 2])\n",
    "x_valid = pd.read_csv('../data/X_valid.csv', index_col=0, header=[0, 1, 2])\n",
    "x_test = pd.read_csv('../data/X_test.csv', index_col=0, header=[0, 1, 2])\n",
    "y_train_cls = pd.read_csv('../data/Y_train.csv') \n",
    "y_valid_cls = pd.read_csv('../data/Y_valid.csv') \n",
    "y_train_los = pd.read_csv('../LoS/Y_train.csv')\n",
    "y_valid_los =  pd.read_csv('../LoS/Y_valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING \n",
    "\n",
    "nulls_data = pd.DataFrame()\n",
    "for col in x_train.columns.get_level_values('LEVEL2').unique():\n",
    "    nulls_data[col] = x_train.loc[:, (col, 'mask')].sum(axis=1).replace({0: np.nan})\n",
    "\n",
    "def filter_min_max(data):\n",
    "    filtered_data = pd.DataFrame()\n",
    "    for col in  nulls_data.columns[nulls_data.isna().sum() <= 5000]:\n",
    "        filtered_data[col+'_min'] = data.loc[:, (col, 'mask')].multiply(data.loc[:, (col, 'mean')]).min(axis=1)\n",
    "        filtered_data[col+'_max'] = data.loc[:, (col, 'mask')].multiply(data.loc[:, (col, 'mean')]).max(axis=1)\n",
    "    return filtered_data.copy()\n",
    "\n",
    "X_train = filter_min_max(x_train)\n",
    "X_valid = filter_min_max(x_valid)\n",
    "X_test = filter_min_max(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_basic(model, hyperparams_list, X_flat_train, X_flat_dev, X_flat_test, target):\n",
    "    if target == \"los_icu\":\n",
    "        y_train = y_train_los\n",
    "        y_valid = y_valid_los\n",
    "    elif target == \"mort_icu\":\n",
    "        y_train = y_train_cls\n",
    "        y_valid = y_valid_cls\n",
    "    else: \n",
    "        raise SystemExit()\n",
    "\n",
    "    best_s, best_hyperparams = -np.Inf, None\n",
    "    for i, hyperparams in enumerate(hyperparams_list):\n",
    "        print(\"On sample %d / %d (hyperparams = %s)\" % (i+1, len(hyperparams_list), repr((hyperparams))))\n",
    "        pipeline = make_pipeline(SimpleImputer(), StandardScaler(), model(**hyperparams))\n",
    "        pipeline.fit(X_flat_train, y_train[target])\n",
    "\n",
    "        s = roc_auc_score(y_valid[target], pipeline.predict_proba(X_flat_dev)[:, 1])\n",
    "        if s > best_s:\n",
    "            best_s, best_hyperparams = s, hyperparams\n",
    "            print(\"New Best Score: %.2f @ hyperparams = %s\" % (100*best_s, repr((best_hyperparams))))\n",
    "\n",
    "    return run_only_final(model, best_hyperparams, X_flat_train, X_flat_dev, X_flat_test, y_train, y_valid, target)\n",
    "\n",
    "\n",
    "def run_only_final(model, best_hyperparams, X_flat_train, X_flat_dev, X_flat_test, y_train, y_valid, target):\n",
    "    pipeline = make_pipeline(SimpleImputer(), StandardScaler(), model(**best_hyperparams))\n",
    "    pipeline.fit(pd.concat((X_flat_train, X_flat_dev)), pd.concat((y_train, y_valid))[target])\n",
    "    \n",
    "    y_score = pipeline.predict_proba(X_flat_test)[:, 1]\n",
    "    y_pred  = pipeline.predict(X_flat_test)\n",
    "\n",
    "    print(\"===== score ===== \\n\", y_score)\n",
    "    print(\"===== pred  ===== \\n\", y_pred)\n",
    "    \n",
    "    return y_score, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification\n",
    "\n",
    "for model, params in [\n",
    "        (SVC, SV_hyperparams_list), \n",
    "        (RandomForestClassifier, RF_hyperparams_list), \n",
    "        (LogisticRegression, LR_hyperparams_list)\n",
    "]:\n",
    "    (score, pred) = run_basic(model, params, X_train, X_valid, X_test, 'mort_icu')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
